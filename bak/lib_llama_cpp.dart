// Copyright (c) 2023, the Dart project authors.  Please see the AUTHORS file
// for details. All rights reserved. Use of this source code is governed by a
// BSD-style license that can be found in the LICENSE file.

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// Helpers for getting default parameters
/// TODO: update API to start accepting pointers to params structs (https://github.com/ggerganov/llama.cpp/discussions/9172)
@ffi.Native<llama_model_params Function()>(symbol: 'llama_model_default_params')
external llama_model_params llama_model_default_params();

@ffi.Native<llama_context_params Function()>(
    symbol: 'llama_context_default_params')
external llama_context_params llama_context_default_params();

@ffi.Native<llama_sampler_chain_params Function()>(
    symbol: 'llama_sampler_chain_default_params')
external llama_sampler_chain_params llama_sampler_chain_default_params();

@ffi.Native<llama_model_quantize_params Function()>(
    symbol: 'llama_model_quantize_default_params')
external llama_model_quantize_params llama_model_quantize_default_params();

/// Initialize the llama + ggml backend
/// If numa is true, use NUMA optimizations
/// Call once at the start of the program
@ffi.Native<ffi.Void Function()>(symbol: 'llama_backend_init')
external void llama_backend_init();

/// Call once at the end of the program - currently only used for MPI
@ffi.Native<ffi.Void Function()>(symbol: 'llama_backend_free')
external void llama_backend_free();

/// optional:
@ffi.Native<ffi.Void Function(ffi.Int32)>(symbol: 'llama_numa_init')
external void llama_numa_init(
  int numa,
);

/// Optional: an auto threadpool gets created in ggml if not passed explicitly
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, ggml_threadpool_t,
        ggml_threadpool_t)>(symbol: 'llama_attach_threadpool')
external void llama_attach_threadpool(
  ffi.Pointer<llama_context> ctx,
  ggml_threadpool_t threadpool,
  ggml_threadpool_t threadpool_batch,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_detach_threadpool')
external void llama_detach_threadpool(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<
    ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>,
        llama_model_params)>(symbol: 'llama_load_model_from_file')
external ffi.Pointer<llama_model> llama_load_model_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from a file
/// If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
/// If the split file name does not follow this pattern, use llama_model_load_from_splits
@ffi.Native<
    ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>,
        llama_model_params)>(symbol: 'llama_model_load_from_file')
external ffi.Pointer<llama_model> llama_model_load_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from multiple splits (support custom naming scheme)
/// The paths must be in the correct order
@ffi.Native<
    ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size, llama_model_params)>(symbol: 'llama_model_load_from_splits')
external ffi.Pointer<llama_model> llama_model_load_from_splits(
  ffi.Pointer<ffi.Pointer<ffi.Char>> paths,
  int n_paths,
  llama_model_params params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_free_model')
external void llama_free_model(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_free')
external void llama_model_free(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<
    ffi.Pointer<llama_context> Function(ffi.Pointer<llama_model>,
        llama_context_params)>(symbol: 'llama_init_from_model')
external ffi.Pointer<llama_context> llama_init_from_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

@ffi.Native<
    ffi.Pointer<llama_context> Function(ffi.Pointer<llama_model>,
        llama_context_params)>(symbol: 'llama_new_context_with_model')
external ffi.Pointer<llama_context> llama_new_context_with_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

/// Frees all allocated memory
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(symbol: 'llama_free')
external void llama_free(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int64 Function()>(symbol: 'llama_time_us')
external int llama_time_us();

@ffi.Native<ffi.Size Function()>(symbol: 'llama_max_devices')
external int llama_max_devices();

@ffi.Native<ffi.Bool Function()>(symbol: 'llama_supports_mmap')
external bool llama_supports_mmap();

@ffi.Native<ffi.Bool Function()>(symbol: 'llama_supports_mlock')
external bool llama_supports_mlock();

@ffi.Native<ffi.Bool Function()>(symbol: 'llama_supports_gpu_offload')
external bool llama_supports_gpu_offload();

@ffi.Native<ffi.Bool Function()>(symbol: 'llama_supports_rpc')
external bool llama_supports_rpc();

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_ctx')
external int llama_n_ctx(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_batch')
external int llama_n_batch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_ubatch')
external int llama_n_ubatch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_seq_max')
external int llama_n_seq_max(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_n_ctx_train')
external int llama_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_n_embd')
external int llama_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_n_layer')
external int llama_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_n_head')
external int llama_n_head(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_n_vocab')
external int llama_n_vocab(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_model')
external ffi.Pointer<llama_model> llama_get_model(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_pooling_type')
external int llama_pooling_type1(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Pointer<llama_vocab> Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_get_vocab')
external ffi.Pointer<llama_vocab> llama_model_get_vocab(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_rope_type')
external int llama_model_rope_type(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_n_ctx_train')
external int llama_model_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_n_embd')
external int llama_model_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_n_layer')
external int llama_model_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_n_head')
external int llama_model_n_head(
  ffi.Pointer<llama_model> model,
);

/// Get the model's RoPE frequency scaling factor
@ffi.Native<ffi.Float Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_rope_freq_scale_train')
external double llama_model_rope_freq_scale_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_type')
external int llama_vocab_type1(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_n_tokens')
external int llama_vocab_n_tokens(
  ffi.Pointer<llama_vocab> vocab,
);

/// Get metadata value as a string by key name
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>, ffi.Size)>(symbol: 'llama_model_meta_val_str')
external int llama_model_meta_val_str(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get the number of metadata key/value pairs
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_meta_count')
external int llama_model_meta_count(
  ffi.Pointer<llama_model> model,
);

/// Get metadata key name by index
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_model>,
        ffi.Int32,
        ffi.Pointer<ffi.Char>,
        ffi.Size)>(symbol: 'llama_model_meta_key_by_index')
external int llama_model_meta_key_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get metadata value as a string by index
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_model>,
        ffi.Int32,
        ffi.Pointer<ffi.Char>,
        ffi.Size)>(symbol: 'llama_model_meta_val_str_by_index')
external int llama_model_meta_val_str_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get a string describing the model type
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
        ffi.Size)>(symbol: 'llama_model_desc')
external int llama_model_desc(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Returns the total size of all the tensors in the model in bytes
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_size')
external int llama_model_size(
  ffi.Pointer<llama_model> model,
);

/// Get the default chat template. Returns nullptr if not available
/// If name is NULL, returns the default chat template
@ffi.Native<
    ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>,
        ffi.Pointer<ffi.Char>)>(symbol: 'llama_model_chat_template')
external ffi.Pointer<ffi.Char> llama_model_chat_template(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> name,
);

/// Returns the total number of parameters in the model
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_n_params')
external int llama_model_n_params(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains an encoder that requires llama_encode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_has_encoder')
external bool llama_model_has_encoder(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains a decoder that requires llama_decode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_has_decoder')
external bool llama_model_has_decoder(
  ffi.Pointer<llama_model> model,
);

/// For encoder-decoder models, this function returns id of the token that must be provided
/// to the decoder to start generating output sequence. For other models, it returns -1.
@ffi.Native<llama_token Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_decoder_start_token')
external int llama_model_decoder_start_token(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model is recurrent (like Mamba, RWKV, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>(
    symbol: 'llama_model_is_recurrent')
external bool llama_model_is_recurrent(
  ffi.Pointer<llama_model> model,
);

/// Returns 0 on success
@ffi.Native<
        ffi.Uint32 Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
            ffi.Pointer<llama_model_quantize_params>)>(
    symbol: 'llama_model_quantize')
external int llama_model_quantize(
  ffi.Pointer<ffi.Char> fname_inp,
  ffi.Pointer<ffi.Char> fname_out,
  ffi.Pointer<llama_model_quantize_params> params,
);

/// Load a LoRA adapter from file
@ffi.Native<
    ffi.Pointer<llama_adapter_lora> Function(ffi.Pointer<llama_model>,
        ffi.Pointer<ffi.Char>)>(symbol: 'llama_adapter_lora_init')
external ffi.Pointer<llama_adapter_lora> llama_adapter_lora_init(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> path_lora,
);

/// Manually free a LoRA adapter
/// Note: loaded adapters will be free when the associated model is deleted
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_adapter_lora>)>(
    symbol: 'llama_adapter_lora_free')
external void llama_adapter_lora_free(
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Add a loaded LoRA adapter to given context
/// This will not modify model's weight
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<llama_adapter_lora>,
        ffi.Float)>(symbol: 'llama_set_adapter_lora')
external int llama_set_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
  double scale,
);

/// Remove a specific LoRA adapter from given context
/// Return -1 if the adapter is not present in the context
@ffi.Native<
    ffi.Int32 Function(ffi.Pointer<llama_context>,
        ffi.Pointer<llama_adapter_lora>)>(symbol: 'llama_rm_adapter_lora')
external int llama_rm_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Remove all LoRA adapters from given context
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_clear_adapter_lora')
external void llama_clear_adapter_lora(
  ffi.Pointer<llama_context> ctx,
);

/// Apply a loaded control vector to a llama_context, or if data is NULL, clear
/// the currently loaded vector.
/// n_embd should be the size of a single layer's control, and data should point
/// to an n_embd x n_layers buffer starting from layer 1.
/// il_start and il_end are the layer range the vector should apply to (both inclusive)
/// See llama_control_vector_load in common to load a control vector.
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Float>,
        ffi.Size,
        ffi.Int32,
        ffi.Int32,
        ffi.Int32)>(symbol: 'llama_apply_adapter_cvec')
external int llama_apply_adapter_cvec(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Float> data,
  int len,
  int n_embd,
  int il_start,
  int il_end,
);

/// Create an empty KV cache view. (use only for debugging purposes)
@ffi.Native<
        llama_kv_cache_view Function(ffi.Pointer<llama_context>, ffi.Int32)>(
    symbol: 'llama_kv_cache_view_init')
external llama_kv_cache_view llama_kv_cache_view_init(
  ffi.Pointer<llama_context> ctx,
  int n_seq_max,
);

/// Free a KV cache view. (use only for debugging purposes)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_kv_cache_view>)>(
    symbol: 'llama_kv_cache_view_free')
external void llama_kv_cache_view_free(
  ffi.Pointer<llama_kv_cache_view> view,
);

/// Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
/// TODO: change signature to llama_kv_cache_view_update(struct llama_kv_cache_view * view, const struct llama_context * ctx)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>,
        ffi.Pointer<llama_kv_cache_view>)>(symbol: 'llama_kv_cache_view_update')
external void llama_kv_cache_view_update(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_kv_cache_view> view,
);

/// Returns the number of tokens in the KV cache (slow, use only for debug)
/// If a KV cell has multiple sequences assigned to it, it will be counted multiple times
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_kv_cache_token_count')
external int llama_get_kv_cache_token_count(
  ffi.Pointer<llama_context> ctx,
);

/// Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_kv_cache_used_cells')
external int llama_get_kv_cache_used_cells(
  ffi.Pointer<llama_context> ctx,
);

/// Clear the KV cache - both cell info is erased and KV data is zeroed
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_kv_cache_clear')
external void llama_kv_cache_clear(
  ffi.Pointer<llama_context> ctx,
);

/// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
/// Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
/// seq_id < 0 : match any sequence
/// p0 < 0     : [0,  p1]
/// p1 < 0     : [p0, inf)
@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
        llama_pos)>(symbol: 'llama_kv_cache_seq_rm')
external bool llama_kv_cache_seq_rm(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
);

/// Copy all tokens that belong to the specified sequence to another sequence
/// Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_seq_id,
        llama_pos, llama_pos)>(symbol: 'llama_kv_cache_seq_cp')
external void llama_kv_cache_seq_cp(
  ffi.Pointer<llama_context> ctx,
  int seq_id_src,
  int seq_id_dst,
  int p0,
  int p1,
);

/// Removes all tokens that do not belong to the specified sequence
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id)>(
    symbol: 'llama_kv_cache_seq_keep')
external void llama_kv_cache_seq_keep(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
/// If the KV cache is RoPEd, the KV data is updated accordingly:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
        llama_pos, llama_pos)>(symbol: 'llama_kv_cache_seq_add')
external void llama_kv_cache_seq_add(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
  int delta,
);

/// Integer division of the positions by factor of `d > 1`
/// If the KV cache is RoPEd, the KV data is updated accordingly:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
        llama_pos, ffi.Int)>(symbol: 'llama_kv_cache_seq_div')
external void llama_kv_cache_seq_div(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int p0,
  int p1,
  int d,
);

/// Returns the largest position present in the KV cache for the specified sequence
@ffi.Native<llama_pos Function(ffi.Pointer<llama_context>, llama_seq_id)>(
    symbol: 'llama_kv_cache_seq_pos_max')
external int llama_kv_cache_seq_pos_max(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Defragment the KV cache
/// This will be applied:
/// - lazily on next llama_decode()
/// - explicitly with llama_kv_cache_update()
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_kv_cache_defrag')
external void llama_kv_cache_defrag(
  ffi.Pointer<llama_context> ctx,
);

/// Apply the KV cache updates (such as K-shifts, defragmentation, etc.)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_kv_cache_update')
external void llama_kv_cache_update(
  ffi.Pointer<llama_context> ctx,
);

/// Check if the context supports KV cache shifting
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_kv_cache_can_shift')
external bool llama_kv_cache_can_shift(
  ffi.Pointer<llama_context> ctx,
);

/// Returns the *actual* size in bytes of the state
/// (logits, embedding and kv_cache)
/// Only use when saving the state, not when restoring it, otherwise the size may be too small.
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_state_get_size')
external int llama_state_get_size(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_state_size')
external int llama_get_state_size(
  ffi.Pointer<llama_context> ctx,
);

/// Copies the state to the specified destination address.
/// Destination needs to have allocated enough memory.
/// Returns the number of bytes copied
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size)>(symbol: 'llama_state_get_data')
external int llama_state_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
);

@ffi.Native<
        ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>(
    symbol: 'llama_copy_state_data')
external int llama_copy_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
);

/// Set the state reading from the specified address
/// Returns the number of bytes read
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size)>(symbol: 'llama_state_set_data')
external int llama_state_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
);

@ffi.Native<
        ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>(
    symbol: 'llama_set_state_data')
external int llama_set_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
);

/// Save/load session file
@ffi.Native<
    ffi.Bool Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>,
        ffi.Size,
        ffi.Pointer<ffi.Size>)>(symbol: 'llama_state_load_file')
external bool llama_state_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
    ffi.Bool Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>,
        ffi.Size,
        ffi.Pointer<ffi.Size>)>(symbol: 'llama_load_session_file')
external bool llama_load_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size)>(symbol: 'llama_state_save_file')
external bool llama_state_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
    ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_token>, ffi.Size)>(symbol: 'llama_save_session_file')
external bool llama_save_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

/// Get the exact size needed to copy the KV cache of a single sequence
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>, llama_seq_id)>(
    symbol: 'llama_state_seq_get_size')
external int llama_state_seq_get_size(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Copy the KV cache of a single sequence into the specified buffer
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size, llama_seq_id)>(symbol: 'llama_state_seq_get_data')
external int llama_state_seq_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
  int seq_id,
);

/// Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
/// Returns:
/// - Positive: Ok
/// - Zero: Failed to load
@ffi.Native<
    ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
        ffi.Size, llama_seq_id)>(symbol: 'llama_state_seq_set_data')
external int llama_state_seq_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
  int dest_seq_id,
);

@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Char>,
        llama_seq_id,
        ffi.Pointer<llama_token>,
        ffi.Size)>(symbol: 'llama_state_seq_save_file')
external int llama_state_seq_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int seq_id,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
    ffi.Size Function(
        ffi.Pointer<llama_context>,
        ffi.Pointer<ffi.Char>,
        llama_seq_id,
        ffi.Pointer<llama_token>,
        ffi.Size,
        ffi.Pointer<ffi.Size>)>(symbol: 'llama_state_seq_load_file')
external int llama_state_seq_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int dest_seq_id,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

/// Return batch for single sequence of tokens
/// The sequence ID will be fixed to 0
/// The position of the tokens will be tracked automatically by llama_decode
///
/// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
@ffi.Native<llama_batch Function(ffi.Pointer<llama_token>, ffi.Int32)>(
    symbol: 'llama_batch_get_one')
external llama_batch llama_batch_get_one(
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
);

/// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
/// Each token can be assigned up to n_seq_max sequence ids
/// The batch has to be freed with llama_batch_free()
/// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
/// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
/// The rest of the llama_batch members are allocated with size n_tokens
/// All members are left uninitialized
@ffi.Native<llama_batch Function(ffi.Int32, ffi.Int32, ffi.Int32)>(
    symbol: 'llama_batch_init')
external llama_batch llama_batch_init(
  int n_tokens,
  int embd,
  int n_seq_max,
);

/// Frees a batch of tokens allocated with llama_batch_init()
@ffi.Native<ffi.Void Function(llama_batch)>(symbol: 'llama_batch_free')
external void llama_batch_free(
  llama_batch batch,
);

/// Processes a batch of tokens with the ecoder part of the encoder-decoder model.
/// Stores the encoder output internally for later use by the decoder cross-attention layers.
/// 0 - success
/// < 0 - error. the KV cache state is restored to the state before this call
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>(
    symbol: 'llama_encode')
external int llama_encode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Positive return values does not mean a fatal error, but rather a warning.
/// 0 - success
/// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
/// < 0 - error. the KV cache state is restored to the state before this call
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>(
    symbol: 'llama_decode')
external int llama_decode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Set the number of threads used for decoding
/// n_threads is the number of threads used for generation (single token)
/// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
@ffi.Native<
        ffi.Void Function(ffi.Pointer<llama_context>, ffi.Int32, ffi.Int32)>(
    symbol: 'llama_set_n_threads')
external void llama_set_n_threads(
  ffi.Pointer<llama_context> ctx,
  int n_threads,
  int n_threads_batch,
);

/// Get the number of threads used for generation of a single token.
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_threads')
external int llama_n_threads(
  ffi.Pointer<llama_context> ctx,
);

/// Get the number of threads used for prompt and batch processing (multiple token).
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_n_threads_batch')
external int llama_n_threads_batch(
  ffi.Pointer<llama_context> ctx,
);

/// Set whether the model is in embeddings mode or not
/// If true, embeddings will be returned but logits will not
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>(
    symbol: 'llama_set_embeddings')
external void llama_set_embeddings(
  ffi.Pointer<llama_context> ctx,
  bool embeddings,
);

/// Set whether to use causal attention or not
/// If set to true, the model will only attend to the past tokens
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>(
    symbol: 'llama_set_causal_attn')
external void llama_set_causal_attn(
  ffi.Pointer<llama_context> ctx,
  bool causal_attn,
);

/// Set abort callback
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
        ffi.Pointer<ffi.Void>)>(symbol: 'llama_set_abort_callback')
external void llama_set_abort_callback(
  ffi.Pointer<llama_context> ctx,
  ggml_abort_callback abort_callback,
  ffi.Pointer<ffi.Void> abort_callback_data,
);

/// Wait until all computations are finished
/// This is automatically done when using one of the functions below to obtain the computation results
/// and is not necessary to call it explicitly in most cases
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_synchronize')
external void llama_synchronize(
  ffi.Pointer<llama_context> ctx,
);

/// Token logits obtained from the last call to llama_decode()
/// The logits for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// Rows: number of tokens for which llama_batch.logits[i] != 0
/// Cols: n_vocab
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_logits')
external ffi.Pointer<ffi.Float> llama_get_logits(
  ffi.Pointer<llama_context> ctx,
);

/// Logits for the ith token. For positive indices, Equivalent to:
/// llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
/// Negative indicies can be used to access logits in reverse order, -1 is the last logit.
/// returns NULL for invalid ids.
@ffi.Native<
    ffi.Pointer<ffi.Float> Function(
        ffi.Pointer<llama_context>, ffi.Int32)>(symbol: 'llama_get_logits_ith')
external ffi.Pointer<ffi.Float> llama_get_logits_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get all output token embeddings.
/// when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
/// the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// shape: [n_outputs*n_embd]
/// Otherwise, returns NULL.
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_get_embeddings')
external ffi.Pointer<ffi.Float> llama_get_embeddings(
  ffi.Pointer<llama_context> ctx,
);

/// Get the embeddings for the ith token. For positive indices, Equivalent to:
/// llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
/// Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
/// shape: [n_embd] (1-dimensional)
/// returns NULL for invalid ids.
@ffi.Native<
        ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)>(
    symbol: 'llama_get_embeddings_ith')
external ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get the embeddings for a sequence id
/// Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
/// when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[1] with the rank of the sequence
/// otherwise: float[n_embd] (1-dimensional)
@ffi.Native<
    ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>,
        llama_seq_id)>(symbol: 'llama_get_embeddings_seq')
external ffi.Pointer<ffi.Float> llama_get_embeddings_seq(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Vocab
@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<llama_vocab>, llama_token)>(symbol: 'llama_vocab_get_text')
external ffi.Pointer<ffi.Char> llama_vocab_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_vocab_get_score')
external double llama_vocab_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_vocab_get_attr')
external int llama_vocab_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_vocab_is_eog')
external bool llama_vocab_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Identify if Token Id is a control token or a render-able token
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_vocab_is_control')
external bool llama_vocab_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Special tokens
@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_bos')
external int llama_vocab_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_eos')
external int llama_vocab_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_eot')
external int llama_vocab_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_sep')
external int llama_vocab_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_nl')
external int llama_vocab_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_pad')
external int llama_vocab_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_get_add_bos')
external bool llama_vocab_get_add_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_get_add_eos')
external bool llama_vocab_get_add_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_pre')
external int llama_vocab_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_suf')
external int llama_vocab_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_mid')
external int llama_vocab_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_pad')
external int llama_vocab_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_rep')
external int llama_vocab_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_fim_sep')
external int llama_vocab_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<
    ffi.Pointer<ffi.Char> Function(
        ffi.Pointer<llama_vocab>, llama_token)>(symbol: 'llama_token_get_text')
external ffi.Pointer<ffi.Char> llama_token_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_token_get_score')
external double llama_token_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_token_get_attr')
external int llama_token_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_token_is_eog')
external bool llama_token_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>(
    symbol: 'llama_token_is_control')
external bool llama_token_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_bos')
external int llama_token_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_eos')
external int llama_token_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_eot')
external int llama_token_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_cls')
external int llama_token_cls(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_sep')
external int llama_token_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_nl')
external int llama_token_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_pad')
external int llama_token_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_add_bos_token')
external bool llama_add_bos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_add_eos_token')
external bool llama_add_eos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_pre')
external int llama_token_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_suf')
external int llama_token_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_mid')
external int llama_token_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_pad')
external int llama_token_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_rep')
external int llama_token_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_token_fim_sep')
external int llama_token_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_vocab_cls')
external int llama_vocab_cls(
  ffi.Pointer<llama_vocab> vocab,
);

/// @details Convert the provided text into tokens.
/// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
/// @return Returns the number of tokens on success, no more than n_tokens_max
/// @return Returns a negative number on failure - the number of tokens that would have been returned
/// @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
/// @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
/// as plaintext. Does not insert a leading space.
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>,
        ffi.Int32,
        ffi.Pointer<llama_token>,
        ffi.Int32,
        ffi.Bool,
        ffi.Bool)>(symbol: 'llama_tokenize')
external int llama_tokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> text,
  int text_len,
  ffi.Pointer<llama_token> tokens,
  int n_tokens_max,
  bool add_special,
  bool parse_special,
);

/// Token Id -> Piece.
/// Uses the vocabulary in the provided context.
/// Does not write null terminator to the buffer.
/// User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
/// @param special If true, special tokens are rendered in the output.
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_vocab>,
        llama_token,
        ffi.Pointer<ffi.Char>,
        ffi.Int32,
        ffi.Int32,
        ffi.Bool)>(symbol: 'llama_token_to_piece')
external int llama_token_to_piece(
  ffi.Pointer<llama_vocab> vocab,
  int token,
  ffi.Pointer<ffi.Char> buf,
  int length,
  int lstrip,
  bool special,
);

/// @details Convert the provided tokens into text (inverse of llama_tokenize()).
/// @param text The char pointer must be large enough to hold the resulting text.
/// @return Returns the number of chars/bytes on success, no more than text_len_max.
/// @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
/// @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
/// @param unparse_special If true, special tokens are rendered in the output.
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<llama_token>,
        ffi.Int32,
        ffi.Pointer<ffi.Char>,
        ffi.Int32,
        ffi.Bool,
        ffi.Bool)>(symbol: 'llama_detokenize')
external int llama_detokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
  ffi.Pointer<ffi.Char> text,
  int text_len_max,
  bool remove_special,
  bool unparse_special,
);

/// Apply chat template. Inspired by hf apply_chat_template() on python.
/// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
/// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
/// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
/// @param chat Pointer to a list of multiple llama_chat_message
/// @param n_msg Number of llama_chat_message in this chat
/// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
/// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
/// @param length The size of the allocated buffer
/// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
@ffi.Native<
    ffi.Int32 Function(
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<llama_chat_message>,
        ffi.Size,
        ffi.Bool,
        ffi.Pointer<ffi.Char>,
        ffi.Int32)>(symbol: 'llama_chat_apply_template')
external int llama_chat_apply_template(
  ffi.Pointer<ffi.Char> tmpl,
  ffi.Pointer<llama_chat_message> chat,
  int n_msg,
  bool add_ass,
  ffi.Pointer<ffi.Char> buf,
  int length,
);

/// Get list of built-in chat templates
@ffi.Native<ffi.Int32 Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, ffi.Size)>(
    symbol: 'llama_chat_builtin_templates')
external int llama_chat_builtin_templates(
  ffi.Pointer<ffi.Pointer<ffi.Char>> output,
  int len,
);

/// mirror of llama_sampler_i:
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler_i>,
        llama_sampler_context_t)>(symbol: 'llama_sampler_init')
external ffi.Pointer<llama_sampler> llama_sampler_init(
  ffi.Pointer<llama_sampler_i> iface,
  llama_sampler_context_t ctx,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_name')
external ffi.Pointer<ffi.Char> llama_sampler_name(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>, llama_token)>(
    symbol: 'llama_sampler_accept')
external void llama_sampler_accept(
  ffi.Pointer<llama_sampler> smpl,
  int token,
);

@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_sampler>,
        ffi.Pointer<llama_token_data_array>)>(symbol: 'llama_sampler_apply')
external void llama_sampler_apply(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_token_data_array> cur_p,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_reset')
external void llama_sampler_reset(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_clone')
external ffi.Pointer<llama_sampler> llama_sampler_clone(
  ffi.Pointer<llama_sampler> smpl,
);

/// important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_free')
external void llama_sampler_free(
  ffi.Pointer<llama_sampler> smpl,
);

/// llama_sampler_chain
/// a type of llama_sampler that can chain multiple samplers one after another
@ffi.Native<ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)>(
    symbol: 'llama_sampler_chain_init')
external ffi.Pointer<llama_sampler> llama_sampler_chain_init(
  llama_sampler_chain_params params,
);

/// important: takes ownership of the sampler object and will free it when llama_sampler_free is called
@ffi.Native<
    ffi.Void Function(ffi.Pointer<llama_sampler>,
        ffi.Pointer<llama_sampler>)>(symbol: 'llama_sampler_chain_add')
external void llama_sampler_chain_add(
  ffi.Pointer<llama_sampler> chain,
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>,
        ffi.Int32)>(symbol: 'llama_sampler_chain_get')
external ffi.Pointer<llama_sampler> llama_sampler_chain_get(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_chain_n')
external int llama_sampler_chain_n(
  ffi.Pointer<llama_sampler> chain,
);

/// after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>,
        ffi.Int32)>(symbol: 'llama_sampler_chain_remove')
external ffi.Pointer<llama_sampler> llama_sampler_chain_remove(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

/// available samplers:
@ffi.Native<ffi.Pointer<llama_sampler> Function()>(
    symbol: 'llama_sampler_init_greedy')
external ffi.Pointer<llama_sampler> llama_sampler_init_greedy();

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Uint32)>(
    symbol: 'llama_sampler_init_dist')
external ffi.Pointer<llama_sampler> llama_sampler_init_dist(
  int seed,
);

@ffi.Native<ffi.Pointer<llama_sampler> Function()>(
    symbol: 'llama_sampler_init_softmax')
external ffi.Pointer<llama_sampler> llama_sampler_init_softmax();

/// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Int32)>(
    symbol: 'llama_sampler_init_top_k')
external ffi.Pointer<llama_sampler> llama_sampler_init_top_k(
  int k,
);

/// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>(
    symbol: 'llama_sampler_init_top_p')
external ffi.Pointer<llama_sampler> llama_sampler_init_top_p(
  double p,
  int min_keep,
);

/// @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>(
    symbol: 'llama_sampler_init_min_p')
external ffi.Pointer<llama_sampler> llama_sampler_init_min_p(
  double p,
  int min_keep,
);

/// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>(
    symbol: 'llama_sampler_init_typical')
external ffi.Pointer<llama_sampler> llama_sampler_init_typical(
  double p,
  int min_keep,
);

/// #details Updates the logits l_i` = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float)>(
    symbol: 'llama_sampler_init_temp')
external ffi.Pointer<llama_sampler> llama_sampler_init_temp(
  double t,
);

/// @details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Float, ffi.Float, ffi.Float)>(symbol: 'llama_sampler_init_temp_ext')
external ffi.Pointer<llama_sampler> llama_sampler_init_temp_ext(
  double t,
  double delta,
  double exponent,
);

/// @details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Float, ffi.Size,
        ffi.Uint32)>(symbol: 'llama_sampler_init_xtc')
external ffi.Pointer<llama_sampler> llama_sampler_init_xtc(
  double p,
  double t,
  int min_keep,
  int seed,
);

/// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Int32, ffi.Uint32, ffi.Float,
        ffi.Float, ffi.Int32)>(symbol: 'llama_sampler_init_mirostat')
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat(
  int n_vocab,
  int seed,
  double tau,
  double eta,
  int m,
);

/// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
        ffi.Pointer<llama_sampler> Function(ffi.Uint32, ffi.Float, ffi.Float)>(
    symbol: 'llama_sampler_init_mirostat_v2')
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat_v2(
  int seed,
  double tau,
  double eta,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>)>(symbol: 'llama_sampler_init_grammar')
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
);

/// @details Lazy grammar sampler, introduced in https://github.com/ggerganov/llama.cpp/pull/9639
/// @param trigger_words A list of words that will trigger the grammar sampler. This may be updated to a loose regex syntax (w/ ^) in a near future.
/// @param trigger_tokens A list of tokens that will trigger the grammar sampler.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Char>,
        ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size,
        ffi.Pointer<llama_token>,
        ffi.Size)>(symbol: 'llama_sampler_init_grammar_lazy')
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
  ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_words,
  int num_trigger_words,
  ffi.Pointer<llama_token> trigger_tokens,
  int num_trigger_tokens,
);

/// NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first.
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Int32, ffi.Float, ffi.Float,
        ffi.Float)>(symbol: 'llama_sampler_init_penalties')
external ffi.Pointer<llama_sampler> llama_sampler_init_penalties(
  int penalty_last_n,
  double penalty_repeat,
  double penalty_freq,
  double penalty_present,
);

/// @details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982
@ffi.Native<
    ffi.Pointer<llama_sampler> Function(
        ffi.Pointer<llama_vocab>,
        ffi.Int32,
        ffi.Float,
        ffi.Float,
        ffi.Int32,
        ffi.Int32,
        ffi.Pointer<ffi.Pointer<ffi.Char>>,
        ffi.Size)>(symbol: 'llama_sampler_init_dry')
external ffi.Pointer<llama_sampler> llama_sampler_init_dry(
  ffi.Pointer<llama_vocab> vocab,
  int n_ctx_train,
  double dry_multiplier,
  double dry_base,
  int dry_allowed_length,
  int dry_penalty_last_n,
  ffi.Pointer<ffi.Pointer<ffi.Char>> seq_breakers,
  int num_breakers,
);

@ffi.Native<
    ffi.Pointer<llama_sampler> Function(ffi.Int32, ffi.Int32,
        ffi.Pointer<llama_logit_bias>)>(symbol: 'llama_sampler_init_logit_bias')
external ffi.Pointer<llama_sampler> llama_sampler_init_logit_bias(
  int n_vocab,
  int n_logit_bias,
  ffi.Pointer<llama_logit_bias> logit_bias,
);

/// this sampler is meant to be used for fill-in-the-middle infilling
/// it's supposed to be used after top_k + top_p sampling
///
/// 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
/// 2. combine probs of tokens that have the same prefix
///
/// example:
///
/// - before:
/// "hel":   0.5
/// "hell":  0.2
/// "hello": 0.1
/// "dummy": 0.1
///
/// - after:
/// "hel":   0.8
/// "dummy": 0.1
///
/// 3. discard non-EOG tokens with low prob
/// 4. if no tokens are left -> pick EOT
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>)>(
    symbol: 'llama_sampler_init_infill')
external ffi.Pointer<llama_sampler> llama_sampler_init_infill(
  ffi.Pointer<llama_vocab> vocab,
);

/// Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_sampler_get_seed')
external int llama_sampler_get_seed(
  ffi.Pointer<llama_sampler> smpl,
);

/// @details Sample and accept a token from the idx-th output of the last evaluation
///
/// Shorthand for:
/// const auto * logits = llama_get_logits_ith(ctx, idx);
/// llama_token_data_array cur_p = { ... init from logits ... };
/// llama_sampler_apply(smpl, &cur_p);
/// auto token = cur_p.data[cur_p.selected].id;
/// llama_sampler_accept(smpl, token);
/// return token;
/// Returns the sampled token
@ffi.Native<
    llama_token Function(ffi.Pointer<llama_sampler>, ffi.Pointer<llama_context>,
        ffi.Int32)>(symbol: 'llama_sampler_sample')
external int llama_sampler_sample(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_context> ctx,
  int idx,
);

/// @details Build a split GGUF final path for this chunk.
/// llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf"
/// Returns the split_path length.
@ffi.Native<
    ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size, ffi.Pointer<ffi.Char>,
        ffi.Int, ffi.Int)>(symbol: 'llama_split_path')
external int llama_split_path(
  ffi.Pointer<ffi.Char> split_path,
  int maxlen,
  ffi.Pointer<ffi.Char> path_prefix,
  int split_no,
  int split_count,
);

/// @details Extract the path prefix from the split_path if and only if the split_no and split_count match.
/// llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0"
/// Returns the split_prefix length.
@ffi.Native<
    ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size, ffi.Pointer<ffi.Char>,
        ffi.Int, ffi.Int)>(symbol: 'llama_split_prefix')
external int llama_split_prefix(
  ffi.Pointer<ffi.Char> split_prefix,
  int maxlen,
  ffi.Pointer<ffi.Char> split_path,
  int split_no,
  int split_count,
);

/// Print system information
@ffi.Native<ffi.Pointer<ffi.Char> Function()>(symbol: 'llama_print_system_info')
external ffi.Pointer<ffi.Char> llama_print_system_info();

/// Set callback for all future logging events.
/// If this is not called, or NULL is supplied, everything is output on stderr.
@ffi.Native<ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>(
    symbol: 'llama_log_set')
external void llama_log_set(
  ggml_log_callback log_callback,
  ffi.Pointer<ffi.Void> user_data,
);

@ffi.Native<llama_perf_context_data Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_perf_context')
external llama_perf_context_data llama_perf_context(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_perf_context_print')
external void llama_perf_context_print(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>(
    symbol: 'llama_perf_context_reset')
external void llama_perf_context_reset(
  ffi.Pointer<llama_context> ctx,
);

/// NOTE: the following work only with samplers constructed via llama_sampler_chain_init
@ffi.Native<llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_perf_sampler')
external llama_perf_sampler_data llama_perf_sampler(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_perf_sampler_print')
external void llama_perf_sampler_print(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>(
    symbol: 'llama_perf_sampler_reset')
external void llama_perf_sampler_reset(
  ffi.Pointer<llama_sampler> chain,
);

/// C interface
///
/// TODO: show sample usage
final class llama_vocab extends ffi.Opaque {}

final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

final class llama_sampler extends ffi.Struct {
  external ffi.Pointer<llama_sampler_i> iface;

  external llama_sampler_context_t ctx;
}

/// user code can implement the interface below in order to create custom llama_sampler
final class llama_sampler_i extends ffi.Struct {
  /// can be NULL
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler> smpl)>>
      name;

  /// can be NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_sampler> smpl, llama_token token)>> accept;

  /// required
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler> smpl,
              ffi.Pointer<llama_token_data_array> cur_p)>> apply;

  /// can be NULL
  external ffi.Pointer<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>>
      reset;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_sampler> smpl)>> clone;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi
      .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>> free;
}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

final class llama_token_data_array extends ffi.Struct {
  /// TODO: consider SoA
  /// NOTE: this pointer can be modified by the samplers
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  /// this is the index in the data array (i.e. not the token id)
  @ffi.Int64()
  external int selected;

  @ffi.Bool()
  external bool sorted;
}

/// TODO: simplify (https://github.com/ggerganov/llama.cpp/pull/9294#pullrequestreview-2286561979)
final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

/// Sampling API
///
/// Sample usage:
///
/// // prepare the sampling chain at the start
/// auto sparams = llama_sampler_chain_default_params();
///
/// llama_sampler * smpl = llama_sampler_chain_init(sparams);
///
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_k(50));
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_p(0.9, 1));
/// llama_sampler_chain_add(smpl, llama_sampler_init_temp (0.8));
///
/// // typically, the chain should end with a sampler such as "greedy", "dist" or "mirostat"
/// // this sampler will be responsible to select the actual token
/// llama_sampler_chain_add(smpl, llama_sampler_init_dist(seed));
///
/// ...
///
/// // decoding loop:
/// while (...) {
/// ...
///
/// llama_decode(ctx, batch);
///
/// // sample from the logits of the last token in the batch
/// const llama_token id = llama_sampler_sample(smpl, ctx, -1);
///
/// // accepting the token updates the internal state of certain samplers (e.g. grammar, repetition, etc.)
/// llama_sampler_accept(smpl, id);
/// ...
/// }
///
/// llama_sampler_free(smpl);
///
/// TODO: In the future, llama_sampler will be utilized to offload the sampling to the backends (e.g. GPU).
typedef llama_sampler_context_t = ffi.Pointer<ffi.Void>;

abstract class llama_vocab_type {
  /// For models without vocab
  static const int LLAMA_VOCAB_TYPE_NONE = 0;

  /// LLaMA tokenizer based on byte-level BPE with byte fallback
  static const int LLAMA_VOCAB_TYPE_SPM = 1;

  /// GPT-2 tokenizer based on byte-level BPE
  static const int LLAMA_VOCAB_TYPE_BPE = 2;

  /// BERT tokenizer based on WordPiece
  static const int LLAMA_VOCAB_TYPE_WPM = 3;

  /// T5 tokenizer based on Unigram
  static const int LLAMA_VOCAB_TYPE_UGM = 4;

  /// RWKV tokenizer based on greedy tokenization
  static const int LLAMA_VOCAB_TYPE_RWKV = 5;
}

/// pre-tokenization types
abstract class llama_vocab_pre_type {
  static const int LLAMA_VOCAB_PRE_TYPE_DEFAULT = 0;
  static const int LLAMA_VOCAB_PRE_TYPE_LLAMA3 = 1;
  static const int LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM = 2;
  static const int LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER = 3;
  static const int LLAMA_VOCAB_PRE_TYPE_FALCON = 4;
  static const int LLAMA_VOCAB_PRE_TYPE_MPT = 5;
  static const int LLAMA_VOCAB_PRE_TYPE_STARCODER = 6;
  static const int LLAMA_VOCAB_PRE_TYPE_GPT2 = 7;
  static const int LLAMA_VOCAB_PRE_TYPE_REFACT = 8;
  static const int LLAMA_VOCAB_PRE_TYPE_COMMAND_R = 9;
  static const int LLAMA_VOCAB_PRE_TYPE_STABLELM2 = 10;
  static const int LLAMA_VOCAB_PRE_TYPE_QWEN2 = 11;
  static const int LLAMA_VOCAB_PRE_TYPE_OLMO = 12;
  static const int LLAMA_VOCAB_PRE_TYPE_DBRX = 13;
  static const int LLAMA_VOCAB_PRE_TYPE_SMAUG = 14;
  static const int LLAMA_VOCAB_PRE_TYPE_PORO = 15;
  static const int LLAMA_VOCAB_PRE_TYPE_CHATGLM3 = 16;
  static const int LLAMA_VOCAB_PRE_TYPE_CHATGLM4 = 17;
  static const int LLAMA_VOCAB_PRE_TYPE_VIKING = 18;
  static const int LLAMA_VOCAB_PRE_TYPE_JAIS = 19;
  static const int LLAMA_VOCAB_PRE_TYPE_TEKKEN = 20;
  static const int LLAMA_VOCAB_PRE_TYPE_SMOLLM = 21;
  static const int LLAMA_VOCAB_PRE_TYPE_CODESHELL = 22;
  static const int LLAMA_VOCAB_PRE_TYPE_BLOOM = 23;
  static const int LLAMA_VOCAB_PRE_TYPE_GPT3_FINNISH = 24;
  static const int LLAMA_VOCAB_PRE_TYPE_EXAONE = 25;
  static const int LLAMA_VOCAB_PRE_TYPE_CHAMELEON = 26;
  static const int LLAMA_VOCAB_PRE_TYPE_MINERVA = 27;
  static const int LLAMA_VOCAB_PRE_TYPE_DEEPSEEK3_LLM = 28;
}

abstract class llama_rope_type {
  static const int LLAMA_ROPE_TYPE_NONE = -1;
  static const int LLAMA_ROPE_TYPE_NORM = 0;
  static const int LLAMA_ROPE_TYPE_NEOX = 2;
  static const int LLAMA_ROPE_TYPE_MROPE = 8;
  static const int LLAMA_ROPE_TYPE_VISION = 24;
}

abstract class llama_token_type {
  static const int LLAMA_TOKEN_TYPE_UNDEFINED = 0;
  static const int LLAMA_TOKEN_TYPE_NORMAL = 1;
  static const int LLAMA_TOKEN_TYPE_UNKNOWN = 2;
  static const int LLAMA_TOKEN_TYPE_CONTROL = 3;
  static const int LLAMA_TOKEN_TYPE_USER_DEFINED = 4;
  static const int LLAMA_TOKEN_TYPE_UNUSED = 5;
  static const int LLAMA_TOKEN_TYPE_BYTE = 6;
}

abstract class llama_token_attr {
  static const int LLAMA_TOKEN_ATTR_UNDEFINED = 0;
  static const int LLAMA_TOKEN_ATTR_UNKNOWN = 1;
  static const int LLAMA_TOKEN_ATTR_UNUSED = 2;
  static const int LLAMA_TOKEN_ATTR_NORMAL = 4;

  /// SPECIAL?
  static const int LLAMA_TOKEN_ATTR_CONTROL = 8;
  static const int LLAMA_TOKEN_ATTR_USER_DEFINED = 16;
  static const int LLAMA_TOKEN_ATTR_BYTE = 32;
  static const int LLAMA_TOKEN_ATTR_NORMALIZED = 64;
  static const int LLAMA_TOKEN_ATTR_LSTRIP = 128;
  static const int LLAMA_TOKEN_ATTR_RSTRIP = 256;
  static const int LLAMA_TOKEN_ATTR_SINGLE_WORD = 512;
}

/// model file types
abstract class llama_ftype {
  static const int LLAMA_FTYPE_ALL_F32 = 0;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_F16 = 1;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0 = 2;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_1 = 3;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q8_0 = 7;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_0 = 8;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_1 = 9;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K = 10;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_S = 11;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_M = 12;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_L = 13;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_S = 14;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_M = 15;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_S = 16;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_M = 17;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q6_K = 18;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XXS = 19;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XS = 20;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K_S = 21;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XS = 22;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XXS = 23;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ1_S = 24;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_NL = 25;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_S = 26;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_M = 27;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_S = 28;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_M = 29;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_XS = 30;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ1_M = 31;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_BF16 = 32;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_TQ1_0 = 36;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_TQ2_0 = 37;

  /// not specified in the model file
  static const int LLAMA_FTYPE_GUESSED = 1024;
}

abstract class llama_rope_scaling_type {
  static const int LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_ROPE_SCALING_TYPE_NONE = 0;
  static const int LLAMA_ROPE_SCALING_TYPE_LINEAR = 1;
  static const int LLAMA_ROPE_SCALING_TYPE_YARN = 2;
  static const int LLAMA_ROPE_SCALING_TYPE_LONGROPE = 3;
  static const int LLAMA_ROPE_SCALING_TYPE_MAX_VALUE = 3;
}

abstract class llama_pooling_type {
  static const int LLAMA_POOLING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_POOLING_TYPE_NONE = 0;
  static const int LLAMA_POOLING_TYPE_MEAN = 1;
  static const int LLAMA_POOLING_TYPE_CLS = 2;
  static const int LLAMA_POOLING_TYPE_LAST = 3;

  /// used by reranking models to attach the classification head to the graph
  static const int LLAMA_POOLING_TYPE_RANK = 4;
}

abstract class llama_attention_type {
  static const int LLAMA_ATTENTION_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_ATTENTION_TYPE_CAUSAL = 0;
  static const int LLAMA_ATTENTION_TYPE_NON_CAUSAL = 1;
}

abstract class llama_split_mode {
  /// single GPU
  static const int LLAMA_SPLIT_MODE_NONE = 0;

  /// split layers and KV across GPUs
  static const int LLAMA_SPLIT_MODE_LAYER = 1;

  /// split layers and KV across GPUs, use tensor parallelism if supported
  static const int LLAMA_SPLIT_MODE_ROW = 2;
}

/// Input data for llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// (if set to NULL, the token position will be tracked automatically by llama_decode)
/// - seq_id : the sequence to which the respective token belongs
/// (if set to NULL, the sequence ID will be assumed to be 0)
/// - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output
/// (if set to NULL, only the logits for last token will be returned)
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  /// TODO: rename this to "output"
  external ffi.Pointer<ffi.Int8> logits;
}

typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

abstract class llama_model_kv_override_type {
  static const int LLAMA_KV_OVERRIDE_TYPE_INT = 0;
  static const int LLAMA_KV_OVERRIDE_TYPE_FLOAT = 1;
  static const int LLAMA_KV_OVERRIDE_TYPE_BOOL = 2;
  static const int LLAMA_KV_OVERRIDE_TYPE_STR = 3;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.Int32()
  external int tag;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  external UnnamedUnion1 unnamed;
}

final class UnnamedUnion1 extends ffi.Union {
  @ffi.Int64()
  external int val_i64;

  @ffi.Double()
  external double val_f64;

  @ffi.Bool()
  external bool val_bool;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> val_str;
}

final class llama_model_params extends ffi.Struct {
  /// NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
  external ffi.Pointer<ggml_backend_dev_t> devices;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.Int32()
  external int split_mode;

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;
}

typedef ggml_backend_dev_t = ffi.Pointer<ggml_backend_device>;

final class ggml_backend_device extends ffi.Opaque {}

typedef llama_progress_callback
    = ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;
typedef llama_progress_callbackFunction = ffi.Bool Function(
    ffi.Float progress, ffi.Pointer<ffi.Void> user_data);
typedef Dartllama_progress_callbackFunction = bool Function(
    double progress, ffi.Pointer<ffi.Void> user_data);

/// NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
/// https://github.com/ggerganov/llama.cpp/pull/7544
final class llama_context_params extends ffi.Struct {
  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// logical maximum batch size that can be submitted to llama_decode
  @ffi.Uint32()
  external int n_batch;

  /// physical maximum batch size
  @ffi.Uint32()
  external int n_ubatch;

  /// max number of sequences (i.e. distinct states for recurrent models)
  @ffi.Uint32()
  external int n_seq_max;

  /// number of threads to use for generation
  @ffi.Int32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Int32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int32()
  external int rope_scaling_type;

  /// whether to pool (sum) embedding results by sequence id
  @ffi.Int32()
  external int pooling_type;

  /// attention type to use for embeddings
  @ffi.Int32()
  external int attention_type;

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// defragment the KV cache if holes/size > thold, < 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache [EXPERIMENTAL]
  @ffi.Int32()
  external int type_k;

  /// data type for V cache [EXPERIMENTAL]
  @ffi.Int32()
  external int type_v;

  /// the llama_decode() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
  @ffi.Bool()
  external bool logits_all;

  /// if true, extract embeddings (together with logits)
  @ffi.Bool()
  external bool embeddings;

  /// whether to offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// whether to use flash attention [EXPERIMENTAL]
  @ffi.Bool()
  external bool flash_attn;

  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// Evaluation callback for each node in the graph (set with ggml_backend_sched_set_eval_callback)
/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;
typedef ggml_backend_sched_eval_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ggml_tensor> t, ffi.Bool ask, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_sched_eval_callbackFunction = bool Function(
    ffi.Pointer<ggml_tensor> t, bool ask, ffi.Pointer<ffi.Void> user_data);

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.Int32()
  external int type;

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.Int32()
  external int op;

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// source tensor and offset for views
  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

/// NOTE: always add types at the end of the enum to keep backward compatibility
abstract class ggml_type {
  static const int GGML_TYPE_F32 = 0;
  static const int GGML_TYPE_F16 = 1;
  static const int GGML_TYPE_Q4_0 = 2;
  static const int GGML_TYPE_Q4_1 = 3;

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 = 5, support has been removed
  static const int GGML_TYPE_Q5_0 = 6;
  static const int GGML_TYPE_Q5_1 = 7;
  static const int GGML_TYPE_Q8_0 = 8;
  static const int GGML_TYPE_Q8_1 = 9;
  static const int GGML_TYPE_Q2_K = 10;
  static const int GGML_TYPE_Q3_K = 11;
  static const int GGML_TYPE_Q4_K = 12;
  static const int GGML_TYPE_Q5_K = 13;
  static const int GGML_TYPE_Q6_K = 14;
  static const int GGML_TYPE_Q8_K = 15;
  static const int GGML_TYPE_IQ2_XXS = 16;
  static const int GGML_TYPE_IQ2_XS = 17;
  static const int GGML_TYPE_IQ3_XXS = 18;
  static const int GGML_TYPE_IQ1_S = 19;
  static const int GGML_TYPE_IQ4_NL = 20;
  static const int GGML_TYPE_IQ3_S = 21;
  static const int GGML_TYPE_IQ2_S = 22;
  static const int GGML_TYPE_IQ4_XS = 23;
  static const int GGML_TYPE_I8 = 24;
  static const int GGML_TYPE_I16 = 25;
  static const int GGML_TYPE_I32 = 26;
  static const int GGML_TYPE_I64 = 27;
  static const int GGML_TYPE_F64 = 28;
  static const int GGML_TYPE_IQ1_M = 29;
  static const int GGML_TYPE_BF16 = 30;

  /// GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
  /// GGML_TYPE_Q4_0_4_8 = 32,
  /// GGML_TYPE_Q4_0_8_8 = 33,
  static const int GGML_TYPE_TQ1_0 = 34;
  static const int GGML_TYPE_TQ2_0 = 35;

  /// GGML_TYPE_IQ4_NL_4_4 = 36,
  /// GGML_TYPE_IQ4_NL_4_8 = 37,
  /// GGML_TYPE_IQ4_NL_8_8 = 38,
  static const int GGML_TYPE_COUNT = 39;
}

final class ggml_backend_buffer extends ffi.Opaque {}

/// available tensor operations:
abstract class ggml_op {
  static const int GGML_OP_NONE = 0;
  static const int GGML_OP_DUP = 1;
  static const int GGML_OP_ADD = 2;
  static const int GGML_OP_ADD1 = 3;
  static const int GGML_OP_ACC = 4;
  static const int GGML_OP_SUB = 5;
  static const int GGML_OP_MUL = 6;
  static const int GGML_OP_DIV = 7;
  static const int GGML_OP_SQR = 8;
  static const int GGML_OP_SQRT = 9;
  static const int GGML_OP_LOG = 10;
  static const int GGML_OP_SIN = 11;
  static const int GGML_OP_COS = 12;
  static const int GGML_OP_SUM = 13;
  static const int GGML_OP_SUM_ROWS = 14;
  static const int GGML_OP_MEAN = 15;
  static const int GGML_OP_ARGMAX = 16;
  static const int GGML_OP_COUNT_EQUAL = 17;
  static const int GGML_OP_REPEAT = 18;
  static const int GGML_OP_REPEAT_BACK = 19;
  static const int GGML_OP_CONCAT = 20;
  static const int GGML_OP_SILU_BACK = 21;

  /// normalize
  static const int GGML_OP_NORM = 22;
  static const int GGML_OP_RMS_NORM = 23;
  static const int GGML_OP_RMS_NORM_BACK = 24;
  static const int GGML_OP_GROUP_NORM = 25;
  static const int GGML_OP_MUL_MAT = 26;
  static const int GGML_OP_MUL_MAT_ID = 27;
  static const int GGML_OP_OUT_PROD = 28;
  static const int GGML_OP_SCALE = 29;
  static const int GGML_OP_SET = 30;
  static const int GGML_OP_CPY = 31;
  static const int GGML_OP_CONT = 32;
  static const int GGML_OP_RESHAPE = 33;
  static const int GGML_OP_VIEW = 34;
  static const int GGML_OP_PERMUTE = 35;
  static const int GGML_OP_TRANSPOSE = 36;
  static const int GGML_OP_GET_ROWS = 37;
  static const int GGML_OP_GET_ROWS_BACK = 38;
  static const int GGML_OP_DIAG = 39;
  static const int GGML_OP_DIAG_MASK_INF = 40;
  static const int GGML_OP_DIAG_MASK_ZERO = 41;
  static const int GGML_OP_SOFT_MAX = 42;
  static const int GGML_OP_SOFT_MAX_BACK = 43;
  static const int GGML_OP_ROPE = 44;
  static const int GGML_OP_ROPE_BACK = 45;
  static const int GGML_OP_CLAMP = 46;
  static const int GGML_OP_CONV_TRANSPOSE_1D = 47;
  static const int GGML_OP_IM2COL = 48;
  static const int GGML_OP_IM2COL_BACK = 49;
  static const int GGML_OP_CONV_TRANSPOSE_2D = 50;
  static const int GGML_OP_POOL_1D = 51;
  static const int GGML_OP_POOL_2D = 52;
  static const int GGML_OP_POOL_2D_BACK = 53;

  /// nearest interpolate
  static const int GGML_OP_UPSCALE = 54;
  static const int GGML_OP_PAD = 55;
  static const int GGML_OP_PAD_REFLECT_1D = 56;
  static const int GGML_OP_ARANGE = 57;
  static const int GGML_OP_TIMESTEP_EMBEDDING = 58;
  static const int GGML_OP_ARGSORT = 59;
  static const int GGML_OP_LEAKY_RELU = 60;
  static const int GGML_OP_FLASH_ATTN_EXT = 61;
  static const int GGML_OP_FLASH_ATTN_BACK = 62;
  static const int GGML_OP_SSM_CONV = 63;
  static const int GGML_OP_SSM_SCAN = 64;
  static const int GGML_OP_WIN_PART = 65;
  static const int GGML_OP_WIN_UNPART = 66;
  static const int GGML_OP_GET_REL_POS = 67;
  static const int GGML_OP_ADD_REL_POS = 68;
  static const int GGML_OP_RWKV_WKV6 = 69;
  static const int GGML_OP_GATED_LINEAR_ATTN = 70;
  static const int GGML_OP_UNARY = 71;
  static const int GGML_OP_MAP_UNARY = 72;
  static const int GGML_OP_MAP_BINARY = 73;
  static const int GGML_OP_MAP_CUSTOM1_F32 = 74;
  static const int GGML_OP_MAP_CUSTOM2_F32 = 75;
  static const int GGML_OP_MAP_CUSTOM3_F32 = 76;
  static const int GGML_OP_MAP_CUSTOM1 = 77;
  static const int GGML_OP_MAP_CUSTOM2 = 78;
  static const int GGML_OP_MAP_CUSTOM3 = 79;
  static const int GGML_OP_CROSS_ENTROPY_LOSS = 80;
  static const int GGML_OP_CROSS_ENTROPY_LOSS_BACK = 81;
  static const int GGML_OP_OPT_STEP_ADAMW = 82;
  static const int GGML_OP_COUNT = 83;
}

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_abort_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction = bool Function(
    ffi.Pointer<ffi.Void> data);

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.Int32()
  external int ftype;

  /// output tensor type
  @ffi.Int32()
  external int output_tensor_type;

  /// token embeddings tensor type
  @ffi.Int32()
  external int token_embedding_type;

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// quantize all tensors to the default type
  @ffi.Bool()
  external bool pure;

  /// quantize to the same number of shards
  @ffi.Bool()
  external bool keep_split;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;

  /// pointer to vector containing overrides
  external ffi.Pointer<ffi.Void> kv_overrides;
}

final class llama_logit_bias extends ffi.Struct {
  @llama_token()
  external int token;

  @ffi.Float()
  external double bias;
}

final class llama_sampler_chain_params extends ffi.Struct {
  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// lora adapter
final class llama_adapter_lora extends ffi.Opaque {}

/// numa strategies
abstract class ggml_numa_strategy {
  static const int GGML_NUMA_STRATEGY_DISABLED = 0;
  static const int GGML_NUMA_STRATEGY_DISTRIBUTE = 1;
  static const int GGML_NUMA_STRATEGY_ISOLATE = 2;
  static const int GGML_NUMA_STRATEGY_NUMACTL = 3;
  static const int GGML_NUMA_STRATEGY_MIRROR = 4;
  static const int GGML_NUMA_STRATEGY_COUNT = 5;
}

typedef ggml_threadpool_t = ffi.Pointer<ggml_threadpool>;

final class ggml_threadpool extends ffi.Opaque {}

/// Information associated with an individual cell in the KV cache view.
final class llama_kv_cache_view_cell extends ffi.Struct {
  /// The position for this cell. Takes KV cache shifts into account.
  /// May be negative if the cell is not populated.
  @llama_pos()
  external int pos;
}

/// An updateable view of the KV cache.
final class llama_kv_cache_view extends ffi.Struct {
  /// Number of KV cache cells. This will be the same as the context size.
  @ffi.Int32()
  external int n_cells;

  /// Maximum number of sequences that can exist in a cell. It's not an error
  /// if there are more sequences in a cell than this value, however they will
  /// not be visible in the view cells_sequences.
  @ffi.Int32()
  external int n_seq_max;

  /// Number of tokens in the cache. For example, if there are two populated
  /// cells, the first with 1 sequence id in it and the second with 2 sequence
  /// ids then you'll have 3 tokens.
  @ffi.Int32()
  external int token_count;

  /// Number of populated cache cells.
  @ffi.Int32()
  external int used_cells;

  /// Maximum contiguous empty slots in the cache.
  @ffi.Int32()
  external int max_contiguous;

  /// Index to the start of the max_contiguous slot range. Can be negative
  /// when cache is full.
  @ffi.Int32()
  external int max_contiguous_idx;

  /// Information for an individual cell.
  external ffi.Pointer<llama_kv_cache_view_cell> cells;

  /// The sequences for each cell. There will be n_seq_max items per cell.
  external ffi.Pointer<llama_seq_id> cells_sequences;
}

/// TODO these functions were sandwiched in the old optimization interface, is there a better place for them?
typedef ggml_log_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_log_callbackFunction = ffi.Void Function(ffi.Int32 level,
    ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_log_callbackFunction = void Function(
    int level, ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);

abstract class ggml_log_level {
  static const int GGML_LOG_LEVEL_NONE = 0;
  static const int GGML_LOG_LEVEL_DEBUG = 1;
  static const int GGML_LOG_LEVEL_INFO = 2;
  static const int GGML_LOG_LEVEL_WARN = 3;
  static const int GGML_LOG_LEVEL_ERROR = 4;

  /// continue previous log
  static const int GGML_LOG_LEVEL_CONT = 5;
}

/// Performance utils
///
/// NOTE: Used by llama.cpp examples, avoid using in third-party apps. Instead, do your own performance measurements.
final class llama_perf_context_data extends ffi.Struct {
  @ffi.Double()
  external double t_start_ms;

  @ffi.Double()
  external double t_load_ms;

  @ffi.Double()
  external double t_p_eval_ms;

  @ffi.Double()
  external double t_eval_ms;

  @ffi.Int32()
  external int n_p_eval;

  @ffi.Int32()
  external int n_eval;
}

final class llama_perf_sampler_data extends ffi.Struct {
  @ffi.Double()
  external double t_sample_ms;

  @ffi.Int32()
  external int n_sample;
}

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_TOKEN_NULL = -1;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_FILE_MAGIC_GGSQ = 1734833009;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 9;

const int LLAMA_STATE_SEQ_MAGIC = 1734833009;

const int LLAMA_STATE_SEQ_VERSION = 2;
