# batched

An implementation of [batched](https://github.com/ggml-org/llama.cpp/tree/master/examples/batched) example of llama.cpp

## Usage

`dart --enable-experiment=native-assets run bin/batched.dart --model model.gguf --n-predict 320 --n-parallel 10`
